{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d8406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import asyncio\n",
    "from langdetect import detect\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cf2d6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#reviews        \u001b[39;00m\n\u001b[0;32m     92\u001b[0m reviews \u001b[38;5;241m=\u001b[39m bp_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReviewText__content\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m reviews_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!121!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([rev\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m rev \u001b[38;5;129;01min\u001b[39;00m reviews \u001b[38;5;28;01mif\u001b[39;00m detect(rev\u001b[38;5;241m.\u001b[39mget_text()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     94\u001b[0m review_list\u001b[38;5;241m.\u001b[39mappend(reviews_str)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# synopsis\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 93\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#reviews        \u001b[39;00m\n\u001b[0;32m     92\u001b[0m reviews \u001b[38;5;241m=\u001b[39m bp_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msection\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReviewText__content\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m reviews_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m!121!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([rev\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m rev \u001b[38;5;129;01min\u001b[39;00m reviews \u001b[38;5;28;01mif\u001b[39;00m detect(rev\u001b[38;5;241m.\u001b[39mget_text()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     94\u001b[0m review_list\u001b[38;5;241m.\u001b[39mappend(reviews_str)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# synopsis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ieyua\\anaconda3\\Lib\\site-packages\\langdetect\\detector_factory.py:129\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    127\u001b[0m init_factory()\n\u001b[0;32m    128\u001b[0m detector \u001b[38;5;241m=\u001b[39m _factory\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[1;32m--> 129\u001b[0m detector\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m detector\u001b[38;5;241m.\u001b[39mdetect()\n",
      "File \u001b[1;32mc:\\Users\\ieyua\\anaconda3\\Lib\\site-packages\\langdetect\\detector.py:112\u001b[0m, in \u001b[0;36mDetector.append\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m pre \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ch\n\u001b[1;32m--> 112\u001b[0m pre \u001b[38;5;241m=\u001b[39m ch\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%ProgramFiles%\\Google\\Chrome\\Application\\chrome.exe\n",
    "username = 'helloworldtesting59@gmail.com'\n",
    "password = 'securepassword'\n",
    "#setting up webdriver\n",
    "webdriver_path = '%ProgramFiles%\\Google\\Chrome\\Application\\chrome.exe'\n",
    "\n",
    "\n",
    "#chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
    "#chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "#loggin in \n",
    "login_url = 'https://www.goodreads.com/ap/signin?language=en_US&openid.assoc_handle=amzn_goodreads_web_na&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.mode=checkid_setup&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&openid.pape.max_auth_age=0&openid.return_to=https%3A%2F%2Fwww.goodreads.com%2Fap-handler%2Fsign-in&siteState=ba99ed98e3cb4430399c0bc24e911cce'\n",
    "driver.get(login_url)\n",
    "#seeing if already logged in. \n",
    "try: \n",
    "    driver.find_element(by.CLASS_NAME, 'gr-h3 gr-h3--noTopMargin')\n",
    "    print('already logged in, skipping login proccess')\n",
    "#fuck this\n",
    "except: \n",
    "    #looking for where to login \n",
    "    username_field = driver.find_element(By.NAME, \"email\")\n",
    "    password_field = driver.find_element(By.NAME, \"password\")\n",
    "    #logging in\n",
    "    username_field.send_keys(username)\n",
    "    password_field.send_keys(password)\n",
    "    password_field.send_keys(Keys.RETURN)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "#beautifulsoup addiiton\n",
    "url = 'https://www.goodreads.com/shelf/show/non-fiction'\n",
    "page_number = 1\n",
    "\n",
    "\n",
    "#creating lists\n",
    "title_list = []\n",
    "author_list = []\n",
    "avgrating_list = []\n",
    "number_of_ratings = []\n",
    "pub_date_list = []\n",
    "review_list = []\n",
    "synopsis_list = []\n",
    "genre_list = []\n",
    "    \n",
    "#page numbers \n",
    "while page_number <= 1:\n",
    "    \n",
    "    #beautifulSoup \n",
    "    page = driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    #what im tryna extract\n",
    "    next_page = soup.find('a', class_ = 'next_page')\n",
    "    titles = soup.find_all('a', class_ = 'bookTitle')\n",
    "    author_name = soup.find_all('a', class_ ='authorName')\n",
    "    avg_rating = soup.find_all('span', class_ ='greyText smallText')\n",
    "    \n",
    "    \n",
    "    #checking to see if I got what i need\n",
    "    for title, author, average_rating in zip(titles, author_name, avg_rating):\n",
    "        #breaking down the avg_rating into rating list, rating avg,number of rating, publish date \n",
    "        clean_list = average_rating.get_text().replace('\\n', '').strip()\n",
    "        result_re = re.split(r'\\s*â€”\\s*', clean_list)\n",
    "        \n",
    "        \n",
    "        title_list.append(title.get_text())\n",
    "        author_list.append(author.get_text())\n",
    "        avgrating_list.append(float(result_re[0].split()[2]))\n",
    "        number_of_ratings.append(int(result_re[1].split()[0].replace(',','')))\n",
    "        try:\n",
    "            publication_date = int(result_re[2].split()[-1])\n",
    "            pub_date_list.append(publication_date)\n",
    "        except ValueError:\n",
    "            pub_date_list.append('errorlololol')\n",
    "\n",
    "    for title in titles:    \n",
    "        #setting up restart loop\n",
    "        max_retries = 3\n",
    "        success = False\n",
    "        #extracting reviews + synopsis\n",
    "        for attempt in range(3):\n",
    "            try: \n",
    "                book_page = 'https://www.goodreads.com/' + title['href']\n",
    "                driver.get(book_page)\n",
    "                bp_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                #reviews        \n",
    "                reviews = bp_soup.find_all('section', class_='ReviewText__content')\n",
    "                reviews_str = '!121!'.join([rev.get_text().strip() for rev in reviews if detect(rev.get_text()) == 'en'])\n",
    "                review_list.append(reviews_str)\n",
    "\n",
    "                # synopsis\n",
    "                synopsis = bp_soup.find('span', class_='Formatted')\n",
    "                synopsis_list.append(synopsis.get_text())\n",
    "\n",
    "                # genres\n",
    "                genre_section = bp_soup.find('div', class_='BookPageMetadataSection__genres')\n",
    "                if genre_section:\n",
    "                    genre_elements = genre_section.find_all('a')\n",
    "                    genres = [genre.get_text(strip=True) for genre in genre_elements]\n",
    "                    genre_list.append(genres)\n",
    "                else: \n",
    "                    print('failed to extract genre info')\n",
    "                    genre_list.append('errorlololol')\n",
    "\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            except Exception as e: \n",
    "                print(f'bad link or error, {title.get_text()} because of {e}, retrying... ')\n",
    "                time.sleep(2)\n",
    "\n",
    "        if not success: \n",
    "            print(f'failed to retrieve data for {title.get_text()}  ')\n",
    "            genre_list.append(\"errorlololol\")\n",
    "            synopsis_list.append('errorlololol')\n",
    "            review_list.append('errorlolololol')\n",
    "\n",
    "            time.sleep(0.6)\n",
    "\n",
    "\n",
    "            \n",
    "    #next page\n",
    "    if next_page:\n",
    "        url = 'https://www.goodreads.com/' + next_page['href']\n",
    "        page_number += 1\n",
    "        print(f'went to page {page_number}')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        break\n",
    "\n",
    "driver.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d731a0a",
   "metadata": {},
   "source": [
    "webscraping done!, from there need to check to make sure the length of all lists are the same so we can establish a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "dictionary = {'Book Title': title_list, \n",
    "            'Author': author_list, \n",
    "            'Average Rating': avgrating_list,\n",
    "            'Total Rating': number_of_ratings,\n",
    "            'Publish Date': pub_date_list, \n",
    "            'Synopsis': synopsis_list,\n",
    "            'Review': review_list, \n",
    "            'Genres': genre_list\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0d15a",
   "metadata": {},
   "source": [
    "fixing data lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce13b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dictionary)\n",
    "df.to_csv('goodreads_nonfiction_books.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9340e0f",
   "metadata": {},
   "source": [
    "#ARCHIVES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
