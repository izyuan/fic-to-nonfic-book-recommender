{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c255437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import langid\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import spacy_fastlang\n",
    "import regex\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe(\"language_detector\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up necessary proccessing things\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/goodreads_fiction.csv')\n",
    "df['Review'] = df['Review'].str.replace(\"121\", \" \")\n",
    "df['Review'] = df['Review'].str.replace(\"\\n\", \" \")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_latin_letters(text): #removes anything other than non latin characters\n",
    "    cleaned_text = ''.join(c if (regex.match(r'\\p{IsLatin}', c) or c.isdigit() or c.isspace() or (c == \"'\" and i < len(text) - 1 and regex.match(r'\\p{IsLatin}', text[i+1]))) else ' ' for i, c in enumerate(str(text)))\n",
    "    return ' '.join(cleaned_text.split())\n",
    "\n",
    "\n",
    "\n",
    "def chunk_text (text, chunk_size=75):\n",
    "    words = text.split()\n",
    "    for i in range (0, len(words), chunk_size):\n",
    "        yield \" \".join(words[i:i+chunk_size])\n",
    "\n",
    "def check_en (text):\n",
    "    doc = nlp(text)\n",
    "    return doc._.language == 'en'\n",
    "\n",
    "def process_item(text):\n",
    "    processed_text = []\n",
    "    for chunk in chunk_text(text):\n",
    "        if check_en(chunk):\n",
    "            processed_text.append(chunk)\n",
    "    return ' '.join(processed_text)\n",
    "\n",
    "def remove_stopwords_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc943b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove all none letters from reviews synopsis and genres\n",
    "columns_to_clean = ['Synopsis','Review','Genres']\n",
    "df[\"Raw Synopsis\"] = df[\"Synopsis\"]\n",
    "for column in columns_to_clean:\n",
    "    df[column] = df[column].apply(remove_non_latin_letters)\n",
    "    \n",
    "for column in columns_to_clean:\n",
    "    try:\n",
    "        df[column] = df[column].str.lower()\n",
    "    except:\n",
    "        print (\"error\")\n",
    "\n",
    "\n",
    "#remove all the failed data from webscraping\n",
    "df_filtered = df[~df.eq(\"errorlololol\").any(axis=1)]\n",
    "df = df_filtered\n",
    "\n",
    "df\n",
    "columns_to_check = ['Review', 'Genres', 'Synopsis']\n",
    "\n",
    "for col in columns_to_check:\n",
    "    if df[col].isna().any():\n",
    "        print(f\"'{col}' column contains NoneType or NaN values.\")\n",
    "    else:\n",
    "        print(f\"'{col}' column does not contain NoneType or NaN values.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_proccess = ['Synopsis', 'Review']\n",
    "\n",
    "\n",
    "    \n",
    "df['Proccessed Synopsis'] = df['Synopsis'].apply(process_item)\n",
    "df['Proccessed Review'] = df['Review'].apply(process_item)\n",
    "\n",
    "df['Proccessed Synopsis'] = df['Proccessed Synopsis'].apply(remove_stopwords_and_lemmatize)\n",
    "df['Proccessed Review'] = df['Proccessed Review'].apply(remove_stopwords_and_lemmatize)\n",
    "    \n",
    "df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a053989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df[\"Proccessed Synopsis\"])\n",
    "\n",
    "for hello in df['Proccessed Synopsis']:\n",
    "    if hello == None:\n",
    "        print(\"there is NaN type in proccessed synopsis\")\n",
    "    else:\n",
    "        print(\"false\")\n",
    "#1241 should be the right answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff3955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data\\cleaned_fic_file', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
